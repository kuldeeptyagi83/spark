{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8ca23f1-14de-4ebe-992f-f5590827e12b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# When not using any schema or catalog just reading from adls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "313da7d7-3f47-41cd-bbb2-5c298eaf135e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List all files at the root of the bucket\n",
    "files = dbutils.fs.ls(\"abfss://raw@databdls.dfs.core.windows.net/\")\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f0665ef-8389-4018-a256-79bbe402a05a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# by defaul read all files in folder irrespective of file type\n",
    "df= spark.read.csv(\"abfss://raw@databdls.dfs.core.windows.net/\")\n",
    "df_all_csv= spark.read.csv(\"abfss://raw@databdls.dfs.core.windows.net/*.csv\")\n",
    "df_csv= spark.read.option(\"header\", True).csv(\"abfss://raw@databdls.dfs.core.windows.net/circuits.csv\")\n",
    "df_csv_with_schema= spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"abfss://raw@databdls.dfs.core.windows.net/circuits.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aa73dfc-0fa8-4f02-96ff-ca99d4148b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3efb74dd-b4fc-4081-9012-8809480abfa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show number of input files Spark processed\n",
    "num_files = df.inputFiles()\n",
    "print(f\"Total files read by Spark: {len(num_files)}\")\n",
    "print(\"Files read:\")\n",
    "for f in num_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79707a7d-776c-4539-915f-4efc51103fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_files = df_all_csv.inputFiles()\n",
    "print(f\"Total files read by Spark: {len(num_files)}\")\n",
    "print(\"Files read:\")\n",
    "for f in num_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "776290a0-f6c1-4c84-8b4f-4271e3d0ed2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_files = df_csv.inputFiles()\n",
    "print(f\"Total files read by Spark: {len(num_files)}\")\n",
    "print(\"Files read:\")\n",
    "for f in num_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb3fe841-115a-41be-aaaf-893b4b89fe9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()\n",
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db7be41-f2b4-4aa1-80b5-da08bf680f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa2e088f-9b32-4dec-afe2-9c58a2965c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eff0dbd5-7517-4740-b224-4919c45b4b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22bb2ed3-a0a5-4910-8f1d-0290d1345bf7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1756968434336}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf1d793b-387f-4d57-960b-cb39c529840a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DATa ingestin using Schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "circuits_schema = StructType(fields=[StructField(\"circuitId\", IntegerType(), False),\n",
    "\n",
    "                                   StructField(\"circuitRef\", StringType(), True),\n",
    "                                   StructField(\"name\", StringType(), True),\n",
    "                                   StructField(\"location\", StringType(), True),\n",
    "                                   StructField(\"country\", StringType(), True),\n",
    "                                   StructField(\"lat\", DoubleType(), True),\n",
    "                                   StructField(\"lng\", DoubleType(), True),\n",
    "                                   StructField(\"alt\", IntegerType(),True),\n",
    "                                #    StructField(\"url\", StringType(), True),\n",
    "                                   ])\n",
    "df_csv_with_schema = spark.read.option(\"header\", True)  \\\n",
    ".option(\"treatEmptyValuesAsNulls\", \"false\") \\\n",
    ".schema(circuits_schema).csv(\"abfss://raw@databdls.dfs.core.windows.net/circuits.csv\")\n",
    "\n",
    "# Check for nulls in 'alt' as spark does not throw error\n",
    "null_alt_count = df_csv_with_schema.filter(col(\"alt\").isNull()).count()\n",
    "\n",
    "# if null_alt_count > 0:\n",
    "#      raise ValueError(f\"'alt' column has {null_alt_count} null values but schema says nullable=False.\")\n",
    "\n",
    "# For IntegerType, DoubleType, stringtype, etc. spark turns empty value as null\n",
    "\n",
    "df_csv_with_schema.na\n",
    "\n",
    "df_csv_with_schema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e23568e-674f-4405-bf03-f71d3d7066b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b2f124d-68df-4f26-9ea8-c3dba836183b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename columns and add columns\n",
    "from pyspark.sql.functions import lit, current_timestamp, col\n",
    "Renamed_extra_df=df_csv_with_schema.withColumnRenamed(\"circuitId\", \"circuit_id\") \\\n",
    ".withColumn(\"ingestionTime\", current_timestamp()) \\\n",
    ".withColumn(\"env\", lit(\"Production\")) \\\n",
    " .select(col(\"circuit_id\"),col(\"ingestionTime\"),col(\"env\"))   \n",
    "Renamed_extra_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aadfb1c7-308d-4c92-b9a8-16faa1f56e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write to parquet file and read from parquet file\n",
    "Renamed_extra_df.write.mode(\"overwrite\").parquet(\"abfss://raw@databdls.dfs.core.windows.net/circuits_processed\")\n",
    "spark.read.parquet(\"abfss://processed@databdls.dfs.core.windows.net/circuits_processed\").show()\n",
    "\n",
    "# by default save as delta table even if it is parquet as we cant save parquet as managed table\n",
    "Renamed_extra_df.write.mode(\"overwrite\").saveAsTable(\"my_managed_parquet_table\") \n",
    "display(spark.read.table(\"my_managed_parquet_table\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d487074b-d742-4ee5-b6ba-e1119d21accf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write delta file \n",
    "Renamed_extra_df.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://processed@databdls.dfs.core.windows.net/circuits_processed_delta\")\n",
    "display(spark.read.format(\"delta\").load(\"abfss://processed@databdls.dfs.core.windows.net/circuits_processed_delta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1178f338-ef8b-475b-b3bd-64027b9682ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"USE CATALOG databdev_silver_formula1_catalog\")\n",
    "spark.sql(\"USE CATALOG dev_silver_formula1_catalog\")\n",
    "spark.sql(\"USE SCHEMA processed_formula1_schema\")\n",
    "Renamed_extra_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"circuits_managed\")\n",
    "display(spark.read.table(\"circuits_managed\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-ingest_circuits_file",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
